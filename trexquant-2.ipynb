{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T16:07:16.783792Z",
     "iopub.status.busy": "2024-12-19T16:07:16.783518Z",
     "iopub.status.idle": "2024-12-19T18:23:08.510457Z",
     "shell.execute_reply": "2024-12-19T18:23:08.509502Z",
     "shell.execute_reply.started": "2024-12-19T16:07:16.783771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[hangman_mlm_high.pt] Epoch 1/100, Loss: 2.6634\n",
      "[hangman_mlm_high.pt] Epoch 2/100, Loss: 2.4635\n",
      "[hangman_mlm_high.pt] Epoch 3/100, Loss: 2.4035\n",
      "[hangman_mlm_high.pt] Epoch 4/100, Loss: 2.3703\n",
      "[hangman_mlm_high.pt] Epoch 5/100, Loss: 2.3483\n",
      "[hangman_mlm_high.pt] Epoch 6/100, Loss: 2.3283\n",
      "[hangman_mlm_high.pt] Epoch 7/100, Loss: 2.3142\n",
      "[hangman_mlm_high.pt] Epoch 8/100, Loss: 2.3019\n",
      "[hangman_mlm_high.pt] Epoch 9/100, Loss: 2.2919\n",
      "[hangman_mlm_high.pt] Epoch 10/100, Loss: 2.2823\n",
      "[hangman_mlm_high.pt] Epoch 11/100, Loss: 2.2754\n",
      "[hangman_mlm_high.pt] Epoch 12/100, Loss: 2.2678\n",
      "[hangman_mlm_high.pt] Epoch 13/100, Loss: 2.2613\n",
      "[hangman_mlm_high.pt] Epoch 14/100, Loss: 2.2542\n",
      "[hangman_mlm_high.pt] Epoch 15/100, Loss: 2.2475\n",
      "[hangman_mlm_high.pt] Epoch 16/100, Loss: 2.2445\n",
      "[hangman_mlm_high.pt] Epoch 17/100, Loss: 2.2378\n",
      "[hangman_mlm_high.pt] Epoch 18/100, Loss: 2.2349\n",
      "[hangman_mlm_high.pt] Epoch 19/100, Loss: 2.2305\n",
      "[hangman_mlm_high.pt] Epoch 20/100, Loss: 2.2251\n",
      "[hangman_mlm_high.pt] Epoch 21/100, Loss: 2.2227\n",
      "[hangman_mlm_high.pt] Epoch 22/100, Loss: 2.2196\n",
      "[hangman_mlm_high.pt] Epoch 23/100, Loss: 2.2156\n",
      "[hangman_mlm_high.pt] Epoch 24/100, Loss: 2.2122\n",
      "[hangman_mlm_high.pt] Epoch 25/100, Loss: 2.2098\n",
      "[hangman_mlm_high.pt] Epoch 26/100, Loss: 2.2064\n",
      "[hangman_mlm_high.pt] Epoch 27/100, Loss: 2.2048\n",
      "[hangman_mlm_high.pt] Epoch 28/100, Loss: 2.1990\n",
      "[hangman_mlm_high.pt] Epoch 29/100, Loss: 2.2012\n",
      "[hangman_mlm_high.pt] Epoch 30/100, Loss: 2.1968\n",
      "[hangman_mlm_high.pt] Epoch 31/100, Loss: 2.1931\n",
      "[hangman_mlm_high.pt] Epoch 32/100, Loss: 2.1903\n",
      "[hangman_mlm_high.pt] Epoch 33/100, Loss: 2.1893\n",
      "[hangman_mlm_high.pt] Epoch 34/100, Loss: 2.1864\n",
      "[hangman_mlm_high.pt] Epoch 35/100, Loss: 2.1827\n",
      "[hangman_mlm_high.pt] Epoch 36/100, Loss: 2.1809\n",
      "[hangman_mlm_high.pt] Epoch 37/100, Loss: 2.1776\n",
      "[hangman_mlm_high.pt] Epoch 38/100, Loss: 2.1780\n",
      "[hangman_mlm_high.pt] Epoch 39/100, Loss: 2.1771\n",
      "[hangman_mlm_high.pt] Epoch 40/100, Loss: 2.1745\n",
      "[hangman_mlm_high.pt] Epoch 41/100, Loss: 2.1706\n",
      "[hangman_mlm_high.pt] Epoch 42/100, Loss: 2.1699\n",
      "[hangman_mlm_high.pt] Epoch 43/100, Loss: 2.1685\n",
      "[hangman_mlm_high.pt] Epoch 44/100, Loss: 2.1669\n",
      "[hangman_mlm_high.pt] Epoch 45/100, Loss: 2.1659\n",
      "[hangman_mlm_high.pt] Epoch 46/100, Loss: 2.1626\n",
      "[hangman_mlm_high.pt] Epoch 47/100, Loss: 2.1621\n",
      "[hangman_mlm_high.pt] Epoch 48/100, Loss: 2.1590\n",
      "[hangman_mlm_high.pt] Epoch 49/100, Loss: 2.1590\n",
      "[hangman_mlm_high.pt] Epoch 50/100, Loss: 2.1568\n",
      "[hangman_mlm_high.pt] Epoch 51/100, Loss: 2.1533\n",
      "[hangman_mlm_high.pt] Epoch 52/100, Loss: 2.1548\n",
      "[hangman_mlm_high.pt] Epoch 53/100, Loss: 2.1543\n",
      "[hangman_mlm_high.pt] Epoch 54/100, Loss: 2.1537\n",
      "[hangman_mlm_high.pt] Epoch 55/100, Loss: 2.1522\n",
      "[hangman_mlm_high.pt] Epoch 56/100, Loss: 2.1500\n",
      "[hangman_mlm_high.pt] Epoch 57/100, Loss: 2.1471\n",
      "[hangman_mlm_high.pt] Epoch 58/100, Loss: 2.1464\n",
      "[hangman_mlm_high.pt] Epoch 59/100, Loss: 2.1451\n",
      "[hangman_mlm_high.pt] Epoch 60/100, Loss: 2.1452\n",
      "[hangman_mlm_high.pt] Epoch 61/100, Loss: 2.1434\n",
      "[hangman_mlm_high.pt] Epoch 62/100, Loss: 2.1415\n",
      "[hangman_mlm_high.pt] Epoch 63/100, Loss: 2.1395\n",
      "[hangman_mlm_high.pt] Epoch 64/100, Loss: 2.1420\n",
      "[hangman_mlm_high.pt] Epoch 65/100, Loss: 2.1396\n",
      "[hangman_mlm_high.pt] Epoch 66/100, Loss: 2.1387\n",
      "[hangman_mlm_high.pt] Epoch 67/100, Loss: 2.1382\n",
      "[hangman_mlm_high.pt] Epoch 68/100, Loss: 2.1352\n",
      "[hangman_mlm_high.pt] Epoch 69/100, Loss: 2.1355\n",
      "[hangman_mlm_high.pt] Epoch 70/100, Loss: 2.1342\n",
      "[hangman_mlm_high.pt] Epoch 71/100, Loss: 2.1323\n",
      "[hangman_mlm_high.pt] Epoch 72/100, Loss: 2.1325\n",
      "[hangman_mlm_high.pt] Epoch 73/100, Loss: 2.1323\n",
      "[hangman_mlm_high.pt] Epoch 74/100, Loss: 2.1301\n",
      "[hangman_mlm_high.pt] Epoch 75/100, Loss: 2.1295\n",
      "[hangman_mlm_high.pt] Epoch 76/100, Loss: 2.1277\n",
      "[hangman_mlm_high.pt] Epoch 77/100, Loss: 2.1286\n",
      "[hangman_mlm_high.pt] Epoch 78/100, Loss: 2.1275\n",
      "[hangman_mlm_high.pt] Epoch 79/100, Loss: 2.1268\n",
      "[hangman_mlm_high.pt] Epoch 80/100, Loss: 2.1242\n",
      "[hangman_mlm_high.pt] Epoch 81/100, Loss: 2.1234\n",
      "[hangman_mlm_high.pt] Epoch 82/100, Loss: 2.1249\n",
      "[hangman_mlm_high.pt] Epoch 83/100, Loss: 2.1245\n",
      "[hangman_mlm_high.pt] Epoch 84/100, Loss: 2.1253\n",
      "[hangman_mlm_high.pt] Epoch 85/100, Loss: 2.1203\n",
      "[hangman_mlm_high.pt] Epoch 86/100, Loss: 2.1196\n",
      "[hangman_mlm_high.pt] Epoch 87/100, Loss: 2.1192\n",
      "[hangman_mlm_high.pt] Epoch 88/100, Loss: 2.1178\n",
      "[hangman_mlm_high.pt] Epoch 89/100, Loss: 2.1197\n",
      "[hangman_mlm_high.pt] Epoch 90/100, Loss: 2.1176\n",
      "[hangman_mlm_high.pt] Epoch 91/100, Loss: 2.1175\n",
      "[hangman_mlm_high.pt] Epoch 92/100, Loss: 2.1176\n",
      "[hangman_mlm_high.pt] Epoch 93/100, Loss: 2.1175\n",
      "[hangman_mlm_high.pt] Epoch 94/100, Loss: 2.1148\n",
      "[hangman_mlm_high.pt] Epoch 95/100, Loss: 2.1139\n",
      "[hangman_mlm_high.pt] Epoch 96/100, Loss: 2.1129\n",
      "[hangman_mlm_high.pt] Epoch 97/100, Loss: 2.1110\n",
      "[hangman_mlm_high.pt] Epoch 98/100, Loss: 2.1115\n",
      "[hangman_mlm_high.pt] Epoch 99/100, Loss: 2.1116\n",
      "[hangman_mlm_high.pt] Epoch 100/100, Loss: 2.1110\n",
      "Model saved to hangman_mlm_high.pt\n",
      "[hangman_mlm_medium.pt] Epoch 1/100, Loss: 2.5875\n",
      "[hangman_mlm_medium.pt] Epoch 2/100, Loss: 2.3045\n",
      "[hangman_mlm_medium.pt] Epoch 3/100, Loss: 2.2230\n",
      "[hangman_mlm_medium.pt] Epoch 4/100, Loss: 2.1761\n",
      "[hangman_mlm_medium.pt] Epoch 5/100, Loss: 2.1435\n",
      "[hangman_mlm_medium.pt] Epoch 6/100, Loss: 2.1170\n",
      "[hangman_mlm_medium.pt] Epoch 7/100, Loss: 2.0964\n",
      "[hangman_mlm_medium.pt] Epoch 8/100, Loss: 2.0783\n",
      "[hangman_mlm_medium.pt] Epoch 9/100, Loss: 2.0624\n",
      "[hangman_mlm_medium.pt] Epoch 10/100, Loss: 2.0494\n",
      "[hangman_mlm_medium.pt] Epoch 11/100, Loss: 2.0384\n",
      "[hangman_mlm_medium.pt] Epoch 12/100, Loss: 2.0246\n",
      "[hangman_mlm_medium.pt] Epoch 13/100, Loss: 2.0180\n",
      "[hangman_mlm_medium.pt] Epoch 14/100, Loss: 2.0075\n",
      "[hangman_mlm_medium.pt] Epoch 15/100, Loss: 1.9952\n",
      "[hangman_mlm_medium.pt] Epoch 16/100, Loss: 1.9893\n",
      "[hangman_mlm_medium.pt] Epoch 17/100, Loss: 1.9826\n",
      "[hangman_mlm_medium.pt] Epoch 18/100, Loss: 1.9768\n",
      "[hangman_mlm_medium.pt] Epoch 19/100, Loss: 1.9731\n",
      "[hangman_mlm_medium.pt] Epoch 20/100, Loss: 1.9638\n",
      "[hangman_mlm_medium.pt] Epoch 21/100, Loss: 1.9604\n",
      "[hangman_mlm_medium.pt] Epoch 22/100, Loss: 1.9544\n",
      "[hangman_mlm_medium.pt] Epoch 23/100, Loss: 1.9478\n",
      "[hangman_mlm_medium.pt] Epoch 24/100, Loss: 1.9445\n",
      "[hangman_mlm_medium.pt] Epoch 25/100, Loss: 1.9394\n",
      "[hangman_mlm_medium.pt] Epoch 26/100, Loss: 1.9326\n",
      "[hangman_mlm_medium.pt] Epoch 27/100, Loss: 1.9289\n",
      "[hangman_mlm_medium.pt] Epoch 28/100, Loss: 1.9266\n",
      "[hangman_mlm_medium.pt] Epoch 29/100, Loss: 1.9207\n",
      "[hangman_mlm_medium.pt] Epoch 30/100, Loss: 1.9161\n",
      "[hangman_mlm_medium.pt] Epoch 31/100, Loss: 1.9152\n",
      "[hangman_mlm_medium.pt] Epoch 32/100, Loss: 1.9097\n",
      "[hangman_mlm_medium.pt] Epoch 33/100, Loss: 1.9055\n",
      "[hangman_mlm_medium.pt] Epoch 34/100, Loss: 1.9039\n",
      "[hangman_mlm_medium.pt] Epoch 35/100, Loss: 1.8990\n",
      "[hangman_mlm_medium.pt] Epoch 36/100, Loss: 1.8976\n",
      "[hangman_mlm_medium.pt] Epoch 37/100, Loss: 1.8930\n",
      "[hangman_mlm_medium.pt] Epoch 38/100, Loss: 1.8900\n",
      "[hangman_mlm_medium.pt] Epoch 39/100, Loss: 1.8869\n",
      "[hangman_mlm_medium.pt] Epoch 40/100, Loss: 1.8846\n",
      "[hangman_mlm_medium.pt] Epoch 41/100, Loss: 1.8810\n",
      "[hangman_mlm_medium.pt] Epoch 42/100, Loss: 1.8793\n",
      "[hangman_mlm_medium.pt] Epoch 43/100, Loss: 1.8759\n",
      "[hangman_mlm_medium.pt] Epoch 44/100, Loss: 1.8722\n",
      "[hangman_mlm_medium.pt] Epoch 45/100, Loss: 1.8716\n",
      "[hangman_mlm_medium.pt] Epoch 46/100, Loss: 1.8652\n",
      "[hangman_mlm_medium.pt] Epoch 47/100, Loss: 1.8665\n",
      "[hangman_mlm_medium.pt] Epoch 48/100, Loss: 1.8619\n",
      "[hangman_mlm_medium.pt] Epoch 49/100, Loss: 1.8610\n",
      "[hangman_mlm_medium.pt] Epoch 50/100, Loss: 1.8610\n",
      "[hangman_mlm_medium.pt] Epoch 51/100, Loss: 1.8561\n",
      "[hangman_mlm_medium.pt] Epoch 52/100, Loss: 1.8557\n",
      "[hangman_mlm_medium.pt] Epoch 53/100, Loss: 1.8545\n",
      "[hangman_mlm_medium.pt] Epoch 54/100, Loss: 1.8545\n",
      "[hangman_mlm_medium.pt] Epoch 55/100, Loss: 1.8483\n",
      "[hangman_mlm_medium.pt] Epoch 56/100, Loss: 1.8465\n",
      "[hangman_mlm_medium.pt] Epoch 57/100, Loss: 1.8417\n",
      "[hangman_mlm_medium.pt] Epoch 58/100, Loss: 1.8430\n",
      "[hangman_mlm_medium.pt] Epoch 59/100, Loss: 1.8416\n",
      "[hangman_mlm_medium.pt] Epoch 60/100, Loss: 1.8386\n",
      "[hangman_mlm_medium.pt] Epoch 61/100, Loss: 1.8390\n",
      "[hangman_mlm_medium.pt] Epoch 62/100, Loss: 1.8372\n",
      "[hangman_mlm_medium.pt] Epoch 63/100, Loss: 1.8317\n",
      "[hangman_mlm_medium.pt] Epoch 64/100, Loss: 1.8333\n",
      "[hangman_mlm_medium.pt] Epoch 65/100, Loss: 1.8315\n",
      "[hangman_mlm_medium.pt] Epoch 66/100, Loss: 1.8288\n",
      "[hangman_mlm_medium.pt] Epoch 67/100, Loss: 1.8300\n",
      "[hangman_mlm_medium.pt] Epoch 68/100, Loss: 1.8280\n",
      "[hangman_mlm_medium.pt] Epoch 69/100, Loss: 1.8254\n",
      "[hangman_mlm_medium.pt] Epoch 70/100, Loss: 1.8242\n",
      "[hangman_mlm_medium.pt] Epoch 71/100, Loss: 1.8217\n",
      "[hangman_mlm_medium.pt] Epoch 72/100, Loss: 1.8233\n",
      "[hangman_mlm_medium.pt] Epoch 73/100, Loss: 1.8194\n",
      "[hangman_mlm_medium.pt] Epoch 74/100, Loss: 1.8170\n",
      "[hangman_mlm_medium.pt] Epoch 75/100, Loss: 1.8129\n",
      "[hangman_mlm_medium.pt] Epoch 76/100, Loss: 1.8152\n",
      "[hangman_mlm_medium.pt] Epoch 77/100, Loss: 1.8101\n",
      "[hangman_mlm_medium.pt] Epoch 78/100, Loss: 1.8113\n",
      "[hangman_mlm_medium.pt] Epoch 79/100, Loss: 1.8118\n",
      "[hangman_mlm_medium.pt] Epoch 80/100, Loss: 1.8080\n",
      "[hangman_mlm_medium.pt] Epoch 81/100, Loss: 1.8081\n",
      "[hangman_mlm_medium.pt] Epoch 82/100, Loss: 1.8083\n",
      "[hangman_mlm_medium.pt] Epoch 83/100, Loss: 1.8040\n",
      "[hangman_mlm_medium.pt] Epoch 84/100, Loss: 1.8069\n",
      "[hangman_mlm_medium.pt] Epoch 85/100, Loss: 1.8031\n",
      "[hangman_mlm_medium.pt] Epoch 86/100, Loss: 1.8033\n",
      "[hangman_mlm_medium.pt] Epoch 87/100, Loss: 1.8009\n",
      "[hangman_mlm_medium.pt] Epoch 88/100, Loss: 1.8011\n",
      "[hangman_mlm_medium.pt] Epoch 89/100, Loss: 1.7978\n",
      "[hangman_mlm_medium.pt] Epoch 90/100, Loss: 1.7989\n",
      "[hangman_mlm_medium.pt] Epoch 91/100, Loss: 1.7982\n",
      "[hangman_mlm_medium.pt] Epoch 92/100, Loss: 1.7931\n",
      "[hangman_mlm_medium.pt] Epoch 93/100, Loss: 1.7936\n",
      "[hangman_mlm_medium.pt] Epoch 94/100, Loss: 1.7927\n",
      "[hangman_mlm_medium.pt] Epoch 95/100, Loss: 1.7931\n",
      "[hangman_mlm_medium.pt] Epoch 96/100, Loss: 1.7922\n",
      "[hangman_mlm_medium.pt] Epoch 97/100, Loss: 1.7903\n",
      "[hangman_mlm_medium.pt] Epoch 98/100, Loss: 1.7889\n",
      "[hangman_mlm_medium.pt] Epoch 99/100, Loss: 1.7846\n",
      "[hangman_mlm_medium.pt] Epoch 100/100, Loss: 1.7878\n",
      "Model saved to hangman_mlm_medium.pt\n",
      "[hangman_mlm_low.pt] Epoch 1/100, Loss: 2.5699\n",
      "[hangman_mlm_low.pt] Epoch 2/100, Loss: 2.2011\n",
      "[hangman_mlm_low.pt] Epoch 3/100, Loss: 2.0873\n",
      "[hangman_mlm_low.pt] Epoch 4/100, Loss: 2.0209\n",
      "[hangman_mlm_low.pt] Epoch 5/100, Loss: 1.9804\n",
      "[hangman_mlm_low.pt] Epoch 6/100, Loss: 1.9431\n",
      "[hangman_mlm_low.pt] Epoch 7/100, Loss: 1.9128\n",
      "[hangman_mlm_low.pt] Epoch 8/100, Loss: 1.8927\n",
      "[hangman_mlm_low.pt] Epoch 9/100, Loss: 1.8735\n",
      "[hangman_mlm_low.pt] Epoch 10/100, Loss: 1.8508\n",
      "[hangman_mlm_low.pt] Epoch 11/100, Loss: 1.8357\n",
      "[hangman_mlm_low.pt] Epoch 12/100, Loss: 1.8232\n",
      "[hangman_mlm_low.pt] Epoch 13/100, Loss: 1.8100\n",
      "[hangman_mlm_low.pt] Epoch 14/100, Loss: 1.7960\n",
      "[hangman_mlm_low.pt] Epoch 15/100, Loss: 1.7818\n",
      "[hangman_mlm_low.pt] Epoch 16/100, Loss: 1.7718\n",
      "[hangman_mlm_low.pt] Epoch 17/100, Loss: 1.7589\n",
      "[hangman_mlm_low.pt] Epoch 18/100, Loss: 1.7520\n",
      "[hangman_mlm_low.pt] Epoch 19/100, Loss: 1.7433\n",
      "[hangman_mlm_low.pt] Epoch 20/100, Loss: 1.7384\n",
      "[hangman_mlm_low.pt] Epoch 21/100, Loss: 1.7259\n",
      "[hangman_mlm_low.pt] Epoch 22/100, Loss: 1.7234\n",
      "[hangman_mlm_low.pt] Epoch 23/100, Loss: 1.7113\n",
      "[hangman_mlm_low.pt] Epoch 24/100, Loss: 1.7072\n",
      "[hangman_mlm_low.pt] Epoch 25/100, Loss: 1.6965\n",
      "[hangman_mlm_low.pt] Epoch 26/100, Loss: 1.6925\n",
      "[hangman_mlm_low.pt] Epoch 27/100, Loss: 1.6875\n",
      "[hangman_mlm_low.pt] Epoch 28/100, Loss: 1.6812\n",
      "[hangman_mlm_low.pt] Epoch 29/100, Loss: 1.6776\n",
      "[hangman_mlm_low.pt] Epoch 30/100, Loss: 1.6667\n",
      "[hangman_mlm_low.pt] Epoch 31/100, Loss: 1.6627\n",
      "[hangman_mlm_low.pt] Epoch 32/100, Loss: 1.6641\n",
      "[hangman_mlm_low.pt] Epoch 33/100, Loss: 1.6563\n",
      "[hangman_mlm_low.pt] Epoch 34/100, Loss: 1.6536\n",
      "[hangman_mlm_low.pt] Epoch 35/100, Loss: 1.6523\n",
      "[hangman_mlm_low.pt] Epoch 36/100, Loss: 1.6427\n",
      "[hangman_mlm_low.pt] Epoch 37/100, Loss: 1.6357\n",
      "[hangman_mlm_low.pt] Epoch 38/100, Loss: 1.6328\n",
      "[hangman_mlm_low.pt] Epoch 39/100, Loss: 1.6334\n",
      "[hangman_mlm_low.pt] Epoch 40/100, Loss: 1.6281\n",
      "[hangman_mlm_low.pt] Epoch 41/100, Loss: 1.6214\n",
      "[hangman_mlm_low.pt] Epoch 42/100, Loss: 1.6211\n",
      "[hangman_mlm_low.pt] Epoch 43/100, Loss: 1.6165\n",
      "[hangman_mlm_low.pt] Epoch 44/100, Loss: 1.6135\n",
      "[hangman_mlm_low.pt] Epoch 45/100, Loss: 1.6044\n",
      "[hangman_mlm_low.pt] Epoch 46/100, Loss: 1.6085\n",
      "[hangman_mlm_low.pt] Epoch 47/100, Loss: 1.6041\n",
      "[hangman_mlm_low.pt] Epoch 48/100, Loss: 1.5980\n",
      "[hangman_mlm_low.pt] Epoch 49/100, Loss: 1.5942\n",
      "[hangman_mlm_low.pt] Epoch 50/100, Loss: 1.5928\n",
      "[hangman_mlm_low.pt] Epoch 51/100, Loss: 1.5900\n",
      "[hangman_mlm_low.pt] Epoch 52/100, Loss: 1.5904\n",
      "[hangman_mlm_low.pt] Epoch 53/100, Loss: 1.5793\n",
      "[hangman_mlm_low.pt] Epoch 54/100, Loss: 1.5786\n",
      "[hangman_mlm_low.pt] Epoch 55/100, Loss: 1.5781\n",
      "[hangman_mlm_low.pt] Epoch 56/100, Loss: 1.5747\n",
      "[hangman_mlm_low.pt] Epoch 57/100, Loss: 1.5710\n",
      "[hangman_mlm_low.pt] Epoch 58/100, Loss: 1.5671\n",
      "[hangman_mlm_low.pt] Epoch 59/100, Loss: 1.5645\n",
      "[hangman_mlm_low.pt] Epoch 60/100, Loss: 1.5669\n",
      "[hangman_mlm_low.pt] Epoch 61/100, Loss: 1.5638\n",
      "[hangman_mlm_low.pt] Epoch 62/100, Loss: 1.5591\n",
      "[hangman_mlm_low.pt] Epoch 63/100, Loss: 1.5581\n",
      "[hangman_mlm_low.pt] Epoch 64/100, Loss: 1.5555\n",
      "[hangman_mlm_low.pt] Epoch 65/100, Loss: 1.5479\n",
      "[hangman_mlm_low.pt] Epoch 66/100, Loss: 1.5532\n",
      "[hangman_mlm_low.pt] Epoch 67/100, Loss: 1.5462\n",
      "[hangman_mlm_low.pt] Epoch 68/100, Loss: 1.5487\n",
      "[hangman_mlm_low.pt] Epoch 69/100, Loss: 1.5433\n",
      "[hangman_mlm_low.pt] Epoch 70/100, Loss: 1.5447\n",
      "[hangman_mlm_low.pt] Epoch 71/100, Loss: 1.5440\n",
      "[hangman_mlm_low.pt] Epoch 72/100, Loss: 1.5377\n",
      "[hangman_mlm_low.pt] Epoch 73/100, Loss: 1.5344\n",
      "[hangman_mlm_low.pt] Epoch 74/100, Loss: 1.5307\n",
      "[hangman_mlm_low.pt] Epoch 75/100, Loss: 1.5315\n",
      "[hangman_mlm_low.pt] Epoch 76/100, Loss: 1.5340\n",
      "[hangman_mlm_low.pt] Epoch 77/100, Loss: 1.5269\n",
      "[hangman_mlm_low.pt] Epoch 78/100, Loss: 1.5285\n",
      "[hangman_mlm_low.pt] Epoch 79/100, Loss: 1.5249\n",
      "[hangman_mlm_low.pt] Epoch 80/100, Loss: 1.5186\n",
      "[hangman_mlm_low.pt] Epoch 81/100, Loss: 1.5189\n",
      "[hangman_mlm_low.pt] Epoch 82/100, Loss: 1.5203\n",
      "[hangman_mlm_low.pt] Epoch 83/100, Loss: 1.5168\n",
      "[hangman_mlm_low.pt] Epoch 84/100, Loss: 1.5183\n",
      "[hangman_mlm_low.pt] Epoch 85/100, Loss: 1.5125\n",
      "[hangman_mlm_low.pt] Epoch 86/100, Loss: 1.5137\n",
      "[hangman_mlm_low.pt] Epoch 87/100, Loss: 1.5079\n",
      "[hangman_mlm_low.pt] Epoch 88/100, Loss: 1.5102\n",
      "[hangman_mlm_low.pt] Epoch 89/100, Loss: 1.5084\n",
      "[hangman_mlm_low.pt] Epoch 90/100, Loss: 1.5071\n",
      "[hangman_mlm_low.pt] Epoch 91/100, Loss: 1.5066\n",
      "[hangman_mlm_low.pt] Epoch 92/100, Loss: 1.5001\n",
      "[hangman_mlm_low.pt] Epoch 93/100, Loss: 1.4968\n",
      "[hangman_mlm_low.pt] Epoch 94/100, Loss: 1.5011\n",
      "[hangman_mlm_low.pt] Epoch 95/100, Loss: 1.4985\n",
      "[hangman_mlm_low.pt] Epoch 96/100, Loss: 1.4983\n",
      "[hangman_mlm_low.pt] Epoch 97/100, Loss: 1.4945\n",
      "[hangman_mlm_low.pt] Epoch 98/100, Loss: 1.4964\n",
      "[hangman_mlm_low.pt] Epoch 99/100, Loss: 1.4970\n",
      "[hangman_mlm_low.pt] Epoch 100/100, Loss: 1.4947\n",
      "Model saved to hangman_mlm_low.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class CharMaskedDataset(Dataset):\n",
    "    def __init__(self, word_list, mask_prob=0.20, max_length=15):\n",
    "        self.word_list = [w for w in word_list if w.isalpha() and len(w) > 0 and len(w) <= max_length]\n",
    "        self.mask_prob = mask_prob\n",
    "        self.max_length = max_length\n",
    "        # Vocab: '_' + '[MASK]' + 'a'...'z'\n",
    "        self.vocab = ['_', '[MASK]'] + list(string.ascii_lowercase)\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word_list[idx]\n",
    "        chars = list(word)\n",
    "        if len(chars) < self.max_length:\n",
    "            chars += ['_'] * (self.max_length - len(chars))\n",
    "\n",
    "        input_ids = []\n",
    "        target_ids = []\n",
    "        for c in chars:\n",
    "            if c == '_':\n",
    "                input_ids.append(self.char_to_idx['_'])\n",
    "                target_ids.append(-100)\n",
    "            else:\n",
    "                if random.random() < self.mask_prob:\n",
    "                    input_ids.append(self.char_to_idx['[MASK]'])\n",
    "                    target_ids.append(self.char_to_idx[c])\n",
    "                else:\n",
    "                    input_ids.append(self.char_to_idx[c])\n",
    "                    target_ids.append(-100)\n",
    "\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2).float() * -(math.log(10000.0)/d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class TransformerMLM(nn.Module):\n",
    "    def __init__(self, vocab_size=28, d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=100):\n",
    "        super(TransformerMLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def train_mlm(dictionary, mask_prob, model_name, epochs=25, batch_size=64, lr=0.001, max_length=15, device='cpu'):\n",
    "    dataset = CharMaskedDataset(dictionary, mask_prob=mask_prob, max_length=max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = TransformerMLM(vocab_size=len(dataset.vocab), d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=20).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for input_ids, targets in dataloader:\n",
    "            input_ids, targets = input_ids.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f\"Model saved to {model_name}\")\n",
    "\n",
    "def main():\n",
    "    dictionary_path = \"../input/train-txt/words_250000_train.txt\"\n",
    "    with open(dictionary_path,\"r\") as f:\n",
    "        full_dictionary = f.read().strip().split()\n",
    "    full_dictionary = [w.lower() for w in full_dictionary if w.isalpha()]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    high_mask_prob = 0.55   # High unknown scenario\n",
    "    medium_mask_prob = 0.35 # Medium unknown scenario\n",
    "    low_mask_prob = 0.15    # Low unknown scenario (near end-game)\n",
    "\n",
    "    train_mlm(full_dictionary, mask_prob=high_mask_prob, model_name=\"hangman_mlm_high.pt\", epochs=100, batch_size=128, lr=0.0001, max_length=15, device=device)\n",
    "    train_mlm(full_dictionary, mask_prob=medium_mask_prob, model_name=\"hangman_mlm_medium.pt\", epochs=100, batch_size=128, lr=0.0001, max_length=15, device=device)\n",
    "    train_mlm(full_dictionary, mask_prob=low_mask_prob, model_name=\"hangman_mlm_low.pt\", epochs=100, batch_size=128, lr=0.0001, max_length=15, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:26:31.259639Z",
     "iopub.status.busy": "2024-12-19T18:26:31.259332Z",
     "iopub.status.idle": "2024-12-19T18:39:18.717088Z",
     "shell.execute_reply": "2024-12-19T18:39:18.715986Z",
     "shell.execute_reply.started": "2024-12-19T18:26:31.259617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Short Model Epoch 1/100, Loss: 2.8486\n",
      "Short Model Epoch 2/100, Loss: 2.6266\n",
      "Short Model Epoch 3/100, Loss: 2.5367\n",
      "Short Model Epoch 4/100, Loss: 2.4869\n",
      "Short Model Epoch 5/100, Loss: 2.4500\n",
      "Short Model Epoch 6/100, Loss: 2.4282\n",
      "Short Model Epoch 7/100, Loss: 2.4143\n",
      "Short Model Epoch 8/100, Loss: 2.3934\n",
      "Short Model Epoch 9/100, Loss: 2.3897\n",
      "Short Model Epoch 10/100, Loss: 2.3806\n",
      "Short Model Epoch 11/100, Loss: 2.3740\n",
      "Short Model Epoch 12/100, Loss: 2.3625\n",
      "Short Model Epoch 13/100, Loss: 2.3533\n",
      "Short Model Epoch 14/100, Loss: 2.3485\n",
      "Short Model Epoch 15/100, Loss: 2.3395\n",
      "Short Model Epoch 16/100, Loss: 2.3321\n",
      "Short Model Epoch 17/100, Loss: 2.3340\n",
      "Short Model Epoch 18/100, Loss: 2.3235\n",
      "Short Model Epoch 19/100, Loss: 2.3225\n",
      "Short Model Epoch 20/100, Loss: 2.3162\n",
      "Short Model Epoch 21/100, Loss: 2.3147\n",
      "Short Model Epoch 22/100, Loss: 2.3095\n",
      "Short Model Epoch 23/100, Loss: 2.3034\n",
      "Short Model Epoch 24/100, Loss: 2.2974\n",
      "Short Model Epoch 25/100, Loss: 2.3008\n",
      "Short Model Epoch 26/100, Loss: 2.2942\n",
      "Short Model Epoch 27/100, Loss: 2.2911\n",
      "Short Model Epoch 28/100, Loss: 2.2884\n",
      "Short Model Epoch 29/100, Loss: 2.2901\n",
      "Short Model Epoch 30/100, Loss: 2.2871\n",
      "Short Model Epoch 31/100, Loss: 2.2806\n",
      "Short Model Epoch 32/100, Loss: 2.2811\n",
      "Short Model Epoch 33/100, Loss: 2.2718\n",
      "Short Model Epoch 34/100, Loss: 2.2766\n",
      "Short Model Epoch 35/100, Loss: 2.2781\n",
      "Short Model Epoch 36/100, Loss: 2.2723\n",
      "Short Model Epoch 37/100, Loss: 2.2661\n",
      "Short Model Epoch 38/100, Loss: 2.2640\n",
      "Short Model Epoch 39/100, Loss: 2.2668\n",
      "Short Model Epoch 40/100, Loss: 2.2607\n",
      "Short Model Epoch 41/100, Loss: 2.2654\n",
      "Short Model Epoch 42/100, Loss: 2.2645\n",
      "Short Model Epoch 43/100, Loss: 2.2568\n",
      "Short Model Epoch 44/100, Loss: 2.2509\n",
      "Short Model Epoch 45/100, Loss: 2.2520\n",
      "Short Model Epoch 46/100, Loss: 2.2564\n",
      "Short Model Epoch 47/100, Loss: 2.2542\n",
      "Short Model Epoch 48/100, Loss: 2.2504\n",
      "Short Model Epoch 49/100, Loss: 2.2488\n",
      "Short Model Epoch 50/100, Loss: 2.2465\n",
      "Short Model Epoch 51/100, Loss: 2.2504\n",
      "Short Model Epoch 52/100, Loss: 2.2412\n",
      "Short Model Epoch 53/100, Loss: 2.2410\n",
      "Short Model Epoch 54/100, Loss: 2.2483\n",
      "Short Model Epoch 55/100, Loss: 2.2352\n",
      "Short Model Epoch 56/100, Loss: 2.2444\n",
      "Short Model Epoch 57/100, Loss: 2.2322\n",
      "Short Model Epoch 58/100, Loss: 2.2390\n",
      "Short Model Epoch 59/100, Loss: 2.2364\n",
      "Short Model Epoch 60/100, Loss: 2.2294\n",
      "Short Model Epoch 61/100, Loss: 2.2289\n",
      "Short Model Epoch 62/100, Loss: 2.2323\n",
      "Short Model Epoch 63/100, Loss: 2.2271\n",
      "Short Model Epoch 64/100, Loss: 2.2289\n",
      "Short Model Epoch 65/100, Loss: 2.2356\n",
      "Short Model Epoch 66/100, Loss: 2.2259\n",
      "Short Model Epoch 67/100, Loss: 2.2274\n",
      "Short Model Epoch 68/100, Loss: 2.2223\n",
      "Short Model Epoch 69/100, Loss: 2.2247\n",
      "Short Model Epoch 70/100, Loss: 2.2198\n",
      "Short Model Epoch 71/100, Loss: 2.2251\n",
      "Short Model Epoch 72/100, Loss: 2.2203\n",
      "Short Model Epoch 73/100, Loss: 2.2164\n",
      "Short Model Epoch 74/100, Loss: 2.2170\n",
      "Short Model Epoch 75/100, Loss: 2.2140\n",
      "Short Model Epoch 76/100, Loss: 2.2162\n",
      "Short Model Epoch 77/100, Loss: 2.2077\n",
      "Short Model Epoch 78/100, Loss: 2.2146\n",
      "Short Model Epoch 79/100, Loss: 2.2091\n",
      "Short Model Epoch 80/100, Loss: 2.2103\n",
      "Short Model Epoch 81/100, Loss: 2.2179\n",
      "Short Model Epoch 82/100, Loss: 2.2112\n",
      "Short Model Epoch 83/100, Loss: 2.2161\n",
      "Short Model Epoch 84/100, Loss: 2.2104\n",
      "Short Model Epoch 85/100, Loss: 2.2047\n",
      "Short Model Epoch 86/100, Loss: 2.2078\n",
      "Short Model Epoch 87/100, Loss: 2.2118\n",
      "Short Model Epoch 88/100, Loss: 2.2005\n",
      "Short Model Epoch 89/100, Loss: 2.2050\n",
      "Short Model Epoch 90/100, Loss: 2.2041\n",
      "Short Model Epoch 91/100, Loss: 2.1988\n",
      "Short Model Epoch 92/100, Loss: 2.1979\n",
      "Short Model Epoch 93/100, Loss: 2.2016\n",
      "Short Model Epoch 94/100, Loss: 2.2044\n",
      "Short Model Epoch 95/100, Loss: 2.1990\n",
      "Short Model Epoch 96/100, Loss: 2.1996\n",
      "Short Model Epoch 97/100, Loss: 2.1952\n",
      "Short Model Epoch 98/100, Loss: 2.1952\n",
      "Short Model Epoch 99/100, Loss: 2.1971\n",
      "Short Model Epoch 100/100, Loss: 2.1901\n",
      "Short word model saved to hangman_mlm_short.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class CharMaskedDataset(Dataset):\n",
    "    def __init__(self, word_list, mask_prob=0.25, max_length=7):\n",
    "        self.word_list = [w for w in word_list if w.isalpha() and len(w)>0 and len(w)<=max_length]\n",
    "        self.mask_prob = mask_prob\n",
    "        self.max_length = max_length\n",
    "        self.vocab = ['_', '[MASK]'] + list(string.ascii_lowercase)\n",
    "        self.char_to_idx = {c:i for i,c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i:c for c,i in self.char_to_idx.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word_list[idx]\n",
    "        chars = list(word)\n",
    "        if len(chars)<self.max_length:\n",
    "            chars += ['_']*(self.max_length - len(chars))\n",
    "\n",
    "        input_ids = []\n",
    "        target_ids = []\n",
    "        for c in chars:\n",
    "            if c=='_':\n",
    "                input_ids.append(self.char_to_idx['_'])\n",
    "                target_ids.append(-100)\n",
    "            else:\n",
    "                if random.random()<self.mask_prob:\n",
    "                    input_ids.append(self.char_to_idx['[MASK]'])\n",
    "                    target_ids.append(self.char_to_idx[c])\n",
    "                else:\n",
    "                    input_ids.append(self.char_to_idx[c])\n",
    "                    target_ids.append(-100)\n",
    "        return torch.tensor(input_ids,dtype=torch.long), torch.tensor(target_ids,dtype=torch.long)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        position=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term=torch.exp((torch.arange(0,d_model,2).float()*-(math.log(10000.0)/d_model)))\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_len=x.size(1)\n",
    "        x=x+self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class TransformerMLM(nn.Module):\n",
    "    def __init__(self, vocab_size=28,d_model=128,nhead=4,num_layers=6,dim_feedforward=512,max_len=10):\n",
    "        super(TransformerMLM,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_enc=SinusoidalPositionalEncoding(d_model,max_len)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward,batch_first=True)\n",
    "        self.transformer=nn.TransformerEncoder(encoder_layer,num_layers)\n",
    "        self.fc=nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,input_ids):\n",
    "        x=self.embedding(input_ids)\n",
    "        x=self.pos_enc(x)\n",
    "        x=self.transformer(x)\n",
    "        logits=self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def main():\n",
    "    dictionary_path=\"../input/train-txt/words_250000_train.txt\"\n",
    "    with open(dictionary_path,\"r\") as f:\n",
    "        full_dictionary=f.read().strip().split()\n",
    "    full_dictionary=[w.lower() for w in full_dictionary if w.isalpha()]\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    dataset=CharMaskedDataset(full_dictionary, mask_prob=0.25, max_length=7)\n",
    "    batch_size=128\n",
    "    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "    model=TransformerMLM(vocab_size=len(dataset.vocab),d_model=128,nhead=4,num_layers=6,dim_feedforward=512,max_len=10).to(device)\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    epochs=100\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses=[]\n",
    "        for input_ids,targets in dataloader:\n",
    "            input_ids,targets=input_ids.to(device),targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits=model(input_ids)\n",
    "            loss=criterion(logits.view(-1,logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        avg_loss=sum(losses)/len(losses)\n",
    "        print(f\"Short Model Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(),\"hangman_mlm_short.pt\")\n",
    "    print(\"Short word model saved to hangman_mlm_short.pt\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import string\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "def vowel_count(clean_word, vowels):\n",
    "    v_count = sum(1 for c in clean_word if c in vowels)\n",
    "    return v_count / len(clean_word) if len(clean_word)>0 else 0.0\n",
    "\n",
    "def func(new_dictionary):\n",
    "    dictx = collections.Counter()\n",
    "    for words in new_dictionary:\n",
    "        temp = collections.Counter(words)\n",
    "        for i in temp:\n",
    "            temp[i] = 1\n",
    "        dictx = dictx + temp\n",
    "    return dictx\n",
    "\n",
    "def func2(n_word_dictionary, clean_word):\n",
    "    new_dictionary = []\n",
    "    l = len(clean_word)\n",
    "    if l in n_word_dictionary:\n",
    "        for dict_word in n_word_dictionary[l]:\n",
    "            if re.fullmatch(clean_word, dict_word):\n",
    "                new_dictionary.append(dict_word)\n",
    "    return new_dictionary\n",
    "\n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result=result\n",
    "        self.code=None\n",
    "        try:\n",
    "            self.type=result[\"error_code\"]\n",
    "        except (KeyError,TypeError):\n",
    "            self.type=\"\"\n",
    "        try:\n",
    "            self.message=result[\"error_description\"]\n",
    "        except (KeyError,TypeError):\n",
    "            try:\n",
    "                self.message=result[\"error\"][\"message\"]\n",
    "                self.code=result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type=result[\"error\"].get(\"type\",\"\")\n",
    "            except (KeyError,TypeError):\n",
    "                try:\n",
    "                    self.message=result[\"error_msg\"]\n",
    "                except (KeyError,TypeError):\n",
    "                    self.message=result\n",
    "        super().__init__(self.message)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=20):\n",
    "        super(SinusoidalPositionalEncoding,self).__init__()\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        position=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term=torch.exp((torch.arange(0,d_model,2).float()*-(math.log(10000.0)/d_model)))\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        pe=pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self,x):\n",
    "        seq_len=x.size(1)\n",
    "        x=x+self.pe[:, :seq_len,:]\n",
    "        return x\n",
    "\n",
    "class TransformerMLM(nn.Module):\n",
    "    def __init__(self, vocab_size=28, d_model=128, nhead=4, num_layers=4, dim_feedforward=256, max_len=20):\n",
    "        super(TransformerMLM,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_enc=SinusoidalPositionalEncoding(d_model,max_len)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward,batch_first=True)\n",
    "        self.transformer=nn.TransformerEncoder(encoder_layer,num_layers)\n",
    "        self.fc=nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,input_ids):\n",
    "        x=self.embedding(input_ids)\n",
    "        x=self.pos_enc(x)\n",
    "        x=self.transformer(x)\n",
    "        logits=self.fc(x)\n",
    "        return logits\n",
    "\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url=self.determine_hangman_url()\n",
    "        self.access_token=access_token\n",
    "        self.session=session or requests.Session()\n",
    "        self.timeout=timeout\n",
    "        self.guessed_letters=[]\n",
    "\n",
    "        full_dictionary_location=\"words_250000_train.txt\"\n",
    "        self.full_dictionary=self.build_dictionary(full_dictionary_location)\n",
    "        self.full_dictionary_common_letter_sorted=collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "\n",
    "        self.current_dictionary=self.full_dictionary[:]\n",
    "        self.n_word_dictionary=self.build_substring_dictionary(self.full_dictionary)\n",
    "\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device=device\n",
    "\n",
    "        self.vocab=['_','[MASK]']+list(string.ascii_lowercase)\n",
    "        self.char_to_idx={c:i for i,c in enumerate(self.vocab)}\n",
    "        self.idx_to_char={i:c for c,i in self.char_to_idx.items()}\n",
    "\n",
    "        # Load all four models:\n",
    "        self.mlm_high=TransformerMLM(vocab_size=len(self.vocab), d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=20).to(device)\n",
    "        self.mlm_medium=TransformerMLM(vocab_size=len(self.vocab), d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=20).to(device)\n",
    "        self.mlm_low=TransformerMLM(vocab_size=len(self.vocab), d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=20).to(device)\n",
    "        self.mlm_short=TransformerMLM(vocab_size=len(self.vocab), d_model=128, nhead=4, num_layers=6, dim_feedforward=512, max_len=10).to(device)\n",
    "\n",
    "        self.mlm_high.load_state_dict(torch.load(\"better_models/hangman_mlm_high.pt\", map_location=device))\n",
    "        self.mlm_medium.load_state_dict(torch.load(\"better_models/hangman_mlm_medium.pt\", map_location=device))\n",
    "        self.mlm_low.load_state_dict(torch.load(\"better_models/hangman_mlm_low (1).pt\", map_location=device))\n",
    "        self.mlm_short.load_state_dict(torch.load(\"better_models/hangman_mlm_short.pt\", map_location=device))\n",
    "\n",
    "        self.mlm_high.eval()\n",
    "        self.mlm_medium.eval()\n",
    "        self.mlm_low.eval()\n",
    "        self.mlm_short.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links=['https://trexsim.com','https://sg.trexsim.com']\n",
    "        data={link:0 for link in links}\n",
    "        for link in links:\n",
    "            requests.get(link)\n",
    "            for i in range(10):\n",
    "                s=time.time()\n",
    "                requests.get(link)\n",
    "                data[link]=time.time()-s\n",
    "        link=sorted(data.items(), key=lambda x:x[1])[0][0]\n",
    "        link+='/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def build_substring_dictionary(self, df):\n",
    "        max_length=max(len(w) for w in df)\n",
    "        n_word_dictionary={i:[] for i in range(3,min(max_length,30)+1)}\n",
    "        for count in range(3,min(max_length,30)+1):\n",
    "            for w in df:\n",
    "                if len(w)>=count:\n",
    "                    for i in range(len(w)-count+1):\n",
    "                        n_word_dictionary[count].append(w[i:i+count])\n",
    "        return n_word_dictionary\n",
    "\n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        with open(dictionary_file_location,\"r\") as f:\n",
    "            full_dictionary=f.read().strip().split()\n",
    "        full_dictionary=[w.lower() for w in full_dictionary if w.isalpha()]\n",
    "        return full_dictionary\n",
    "\n",
    "    def try_func2(self, clean_word):\n",
    "        ndict=func2(self.n_word_dictionary, clean_word)\n",
    "        return func(ndict)\n",
    "\n",
    "    def try_substring_logic(self, clean_word, divisor):\n",
    "        length=len(clean_word)\n",
    "        x=int(length/divisor)\n",
    "        c=collections.Counter()\n",
    "        if x>=3:\n",
    "            for i in range(length - x +1):\n",
    "                s=clean_word[i:i+x]\n",
    "                ndict=func2(self.n_word_dictionary,s)\n",
    "                temp=func(ndict)\n",
    "                c=c+temp\n",
    "        return c\n",
    "\n",
    "    def get_model_probs(self, model, clean_word):\n",
    "        mlm_input_chars=[]\n",
    "        for c in clean_word:\n",
    "            if c=='.':\n",
    "                mlm_input_chars.append('[MASK]')\n",
    "            elif c in self.vocab:\n",
    "                mlm_input_chars.append(c)\n",
    "            else:\n",
    "                mlm_input_chars.append('_')\n",
    "\n",
    "\n",
    "        input_ids=[self.char_to_idx.get(ch,self.char_to_idx['_']) for ch in mlm_input_chars]\n",
    "\n",
    "        model_max_len = model.pos_enc.pe.size(1)\n",
    "        if len(input_ids) > model_max_len:\n",
    "            input_ids = input_ids[:model_max_len]\n",
    "            \n",
    "        input_ids_tensor=torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits=model(input_ids_tensor)\n",
    "            mask_positions=[i for i,ch in enumerate(mlm_input_chars) if ch=='[MASK]']\n",
    "            letter_indices=range(2,28)\n",
    "            letter_probs=collections.Counter()\n",
    "            if mask_positions:\n",
    "                for pos in mask_positions:\n",
    "                    pos_logits=logits[0,pos]\n",
    "                    pos_probs=torch.softmax(pos_logits,dim=0)\n",
    "                    for li in letter_indices:\n",
    "                        ltr=self.idx_to_char[li]\n",
    "                        letter_probs[ltr]+=pos_probs[li].item()\n",
    "                for ltr in letter_probs:\n",
    "                    letter_probs[ltr]=letter_probs[ltr]/len(mask_positions)\n",
    "            else:\n",
    "                for ltr in string.ascii_lowercase:\n",
    "                    letter_probs[ltr]=1.0/26.0\n",
    "        return dict(letter_probs)\n",
    "\n",
    "    def guess(self, word):\n",
    "        vowels=set('aeiou')\n",
    "        clean_word=word[::2].replace(\"_\",\".\")\n",
    "        len_word=len(clean_word)\n",
    "        num_unknown=clean_word.count('.')\n",
    "        unknown_ratio=num_unknown/len_word if len_word>0 else 0.0\n",
    "\n",
    "        # Heuristic approach\n",
    "        regex_pattern=\"^\"+clean_word+\"$\"\n",
    "        new_dictionary=[w for w in self.current_dictionary if len(w)==len_word and re.fullmatch(regex_pattern,w)]\n",
    "        self.current_dictionary=new_dictionary\n",
    "\n",
    "        def get_heuristic_letters(clean_word):\n",
    "            c=func(self.current_dictionary)\n",
    "            if len(self.current_dictionary)==0:\n",
    "                c=self.try_func2(clean_word)\n",
    "                if sum(c.values())==0:\n",
    "                    c=self.try_substring_logic(clean_word,2)\n",
    "                    if sum(c.values())==0:\n",
    "                        c=self.try_substring_logic(clean_word,3)\n",
    "                        if sum(c.values())==0:\n",
    "                            c=collections.Counter()\n",
    "                            total=sum(freq for _,freq in self.full_dictionary_common_letter_sorted)\n",
    "                            for ltr,freq in self.full_dictionary_common_letter_sorted:\n",
    "                                c[ltr]=freq\n",
    "            return c\n",
    "\n",
    "        def letter_frequency_to_scores(c):\n",
    "            total=sum(c.values())\n",
    "            scores={}\n",
    "            if total>0:\n",
    "                for ltr,val in c.items():\n",
    "                    scores[ltr]=val/total\n",
    "            return scores\n",
    "\n",
    "        h_counts=get_heuristic_letters(clean_word)\n",
    "        heuristic_scores=letter_frequency_to_scores(h_counts)\n",
    "\n",
    "        if len_word <=7:\n",
    "            # For short words, get short model probs\n",
    "            model_short_prob=self.get_model_probs(self.mlm_short, clean_word)\n",
    "            model_high_prob=self.get_model_probs(self.mlm_high, clean_word)\n",
    "            model_medium_prob=self.get_model_probs(self.mlm_medium, clean_word)\n",
    "            model_low_prob=self.get_model_probs(self.mlm_low, clean_word)\n",
    "\n",
    "            if unknown_ratio >0.7:\n",
    "                w_short=0.1\n",
    "                w_heuristic=0.4\n",
    "                w_high = 0.2\n",
    "                w_medium= 0.15\n",
    "                w_low=0.15\n",
    "            elif unknown_ratio >0.4:\n",
    "                w_short=0.2\n",
    "                w_heuristic=0.4\n",
    "                w_low=0.2\n",
    "                w_high=0.0\n",
    "                w_medium=0.2\n",
    "            elif unknown_ratio >0.15:\n",
    "                w_short=0.2\n",
    "                w_heuristic=0.4\n",
    "                w_low=0.3\n",
    "                w_high=0.0\n",
    "                w_medium=0.1\n",
    "            else:\n",
    "                w_heuristic=0.5\n",
    "                w_short=0.2\n",
    "                w_low=0.2\n",
    "                w_high=0.0\n",
    "                w_medium=0.1\n",
    "\n",
    "            combined_scores={}\n",
    "            for ltr in string.ascii_lowercase:\n",
    "                pm_s=model_short_prob.get(ltr,0.0)\n",
    "                ph=heuristic_scores.get(ltr,0.0)\n",
    "                pm_h=model_high_prob.get(ltr,0.0)\n",
    "                pm_m=model_medium_prob.get(ltr,0.0)\n",
    "                pm_l=model_low_prob.get(ltr,0.0)\n",
    "\n",
    "                combined_scores[ltr]=(w_short*pm_s + w_heuristic*ph + w_high*pm_h + w_medium*pm_m + w_low*pm_l)\n",
    "        else:\n",
    "            model_high_prob=self.get_model_probs(self.mlm_high, clean_word)\n",
    "            model_medium_prob=self.get_model_probs(self.mlm_medium, clean_word)\n",
    "            model_low_prob=self.get_model_probs(self.mlm_low, clean_word)\n",
    "\n",
    "            if unknown_ratio>0.7:\n",
    "                w_high=0.2\n",
    "                w_medium=0.3\n",
    "                w_low=0.2\n",
    "                w_heuristic=0.3\n",
    "            elif unknown_ratio>0.4:\n",
    "                w_high=0.15\n",
    "                w_medium=0.4\n",
    "                w_low=0.15\n",
    "                w_heuristic=0.3\n",
    "            elif unknown_ratio>0.15:\n",
    "                w_high=0.1\n",
    "                w_medium=0.2\n",
    "                w_low=0.3\n",
    "                w_heuristic=0.4\n",
    "            else:\n",
    "                w_high=0.05\n",
    "                w_medium=0.15\n",
    "                w_low=0.3\n",
    "                w_heuristic=0.5\n",
    "\n",
    "            combined_scores={}\n",
    "            for ltr in string.ascii_lowercase:\n",
    "                pm_h=model_high_prob.get(ltr,0.0)\n",
    "                pm_m=model_medium_prob.get(ltr,0.0)\n",
    "                pm_l=model_low_prob.get(ltr,0.0)\n",
    "                ph=heuristic_scores.get(ltr,0.0)\n",
    "                combined_scores[ltr]=(w_high*pm_h + w_medium*pm_m + w_low*pm_l + w_heuristic*ph)\n",
    "\n",
    "        known_letters=[c for c in clean_word if c!='.']\n",
    "        vowels=set('aeiou')\n",
    "        v_ratio=vowel_count(known_letters,vowels) if known_letters else 0.0\n",
    "\n",
    "        sorted_letters=sorted(combined_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        chosen_letter=None\n",
    "        if v_ratio>0.6 and unknown_ratio<0.5:\n",
    "            for letter,score in sorted_letters:\n",
    "                if letter not in self.guessed_letters and letter not in vowels:\n",
    "                    chosen_letter=letter\n",
    "                    break\n",
    "            if chosen_letter is None:\n",
    "                for letter,score in sorted_letters:\n",
    "                    if letter not in self.guessed_letters:\n",
    "                        chosen_letter=letter\n",
    "                        break\n",
    "        else:\n",
    "            for letter, score in sorted_letters:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    chosen_letter=letter\n",
    "                    break\n",
    "\n",
    "        if chosen_letter is None:\n",
    "            chosen_letter='e'\n",
    "\n",
    "        return chosen_letter\n",
    "\n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.guessed_letters=[]\n",
    "        self.current_dictionary=self.full_dictionary.copy()\n",
    "\n",
    "        response=self.request(\"/new_game\",{\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id=response.get('game_id')\n",
    "            word=response.get('word')\n",
    "            tries_remains=response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(f\"Game {game_id} started. Tries:{tries_remains}, Word:{word}\")\n",
    "            while tries_remains>0:\n",
    "                guess_letter=self.guess(word)\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing:\",guess_letter)\n",
    "                try:\n",
    "                    res=self.request(\"/guess_letter\",{\"request\":\"guess_letter\",\"game_id\":game_id,\"letter\":guess_letter})\n",
    "                except HangmanAPIError as e:\n",
    "                    print(\"HangmanAPIError:\",e)\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(\"Unexpected error:\",e)\n",
    "                    raise e\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"Server response:\",res)\n",
    "                status=res.get('status')\n",
    "                tries_remains=res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(f\"Successfully finished game: {game_id}\")\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason=res.get('reason','# tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(f\"Failed game: {game_id}, Reason:{reason}\")\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word=res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return response.get('status')==\"success\"\n",
    "\n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\",{})\n",
    "\n",
    "    def request(self,path,args=None,post_args=None,method=None):\n",
    "        if args is None:\n",
    "            args={}\n",
    "        if post_args is not None:\n",
    "            method=\"POST\"\n",
    "\n",
    "        if self.access_token:\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"]=self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"]=self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "        num_retry,time_sleep=50,2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response=self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url+path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                try:\n",
    "                    resp=e.response.json()\n",
    "                except ValueError:\n",
    "                    resp={\"error_msg\":str(e)}\n",
    "                raise HangmanAPIError(resp)\n",
    "            except requests.exceptions.SSLError:\n",
    "                if it+1==num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "            except requests.exceptions.RequestException:\n",
    "                if it+1==num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers=response.headers\n",
    "        if 'json' in headers.get('content-type',''):\n",
    "            result=response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str=parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result={\"access_token\":query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"]=query_str[\"expires\"][0]\n",
    "            else:\n",
    "                try:\n",
    "                    result=response.json()\n",
    "                except ValueError:\n",
    "                    result={'error_msg':response.text}\n",
    "                raise HangmanAPIError(result)\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text or querystring')\n",
    "\n",
    "        if result and isinstance(result,dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/hdkky04n2t1dq_8gqccxyxf00000gn/T/ipykernel_48793/52893626.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mlm_high.load_state_dict(torch.load(\"better_models/hangman_mlm_high.pt\", map_location=device))\n",
      "/var/folders/37/hdkky04n2t1dq_8gqccxyxf00000gn/T/ipykernel_48793/52893626.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mlm_medium.load_state_dict(torch.load(\"better_models/hangman_mlm_medium.pt\", map_location=device))\n",
      "/var/folders/37/hdkky04n2t1dq_8gqccxyxf00000gn/T/ipykernel_48793/52893626.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mlm_low.load_state_dict(torch.load(\"better_models/hangman_mlm_low (1).pt\", map_location=device))\n",
      "/var/folders/37/hdkky04n2t1dq_8gqccxyxf00000gn/T/ipykernel_48793/52893626.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mlm_short.load_state_dict(torch.load(\"better_models/hangman_mlm_short.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "api = HangmanAPI(access_token=\"7d96983667363dce2c8ee97995455f\", timeout=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n",
      "1008\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status()\n",
    "print(total_practice_runs)\n",
    "print(total_practice_successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  273  th game\n"
     ]
    },
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'Your account has been deactivated!'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m273\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# because 272 games are already played\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mmy_status() \u001b[38;5;66;03m# Get my game stats: (# of tries, # of wins)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m success_rate \u001b[38;5;241m=\u001b[39m total_recorded_successes\u001b[38;5;241m/\u001b[39m(total_recorded_runs)\n",
      "Cell \u001b[0;32mIn[16], line 359\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 359\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    361\u001b[0m     game_id\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 463\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result,\u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'Your account has been deactivated!'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', (i+1), ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "    success_rate = total_recorded_successes/(total_recorded_runs)\n",
    "    print('overall success rate = %.3f' % success_rate)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.568\n",
      "total_practice_runs =  1973\n",
      "total_recorded_runs =  1000\n",
      "total_recorded_successes =  568\n",
      "total_practice_successes =  1008\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/(total_recorded_runs + 1e-6)\n",
    "print('overall success rate = %.3f' % success_rate)\n",
    "\n",
    "print('total_practice_runs = ', total_practice_runs)\n",
    "print('total_recorded_runs = ', total_recorded_runs)\n",
    "print('total_recorded_successes = ', total_recorded_successes)\n",
    "print('total_practice_successes = ', total_practice_successes)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6332900,
     "sourceId": 10240657,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
